{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Predict the running times of prospective Olympic sprinters using data from the last 20 Olympics.\n",
    "\n",
    "Essentially this is a multiple regression \"travel time\" problem: how long will it take athlete \"A\" to get to point \"X\".  Most of the features should be continuous (wind speed, temperature, past performance, height, weight, altitude, longitude, latitude,  etc.) as well as the \"Y\" variable (running time). There should be very few missing values, and the dataset is not huge.  Also, their shouldn't be a huge spread of the data (at the olympic level most athletes should be close to the mean).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. You have more features (columns) than rows in your dataset.\n",
    "\n",
    "In instances like this overfitting would occure with plain linear or logistic regression.\n",
    "\n",
    "From everything I've read, ridge or lasso regression would be the models to use in this case, but then I've also read that random forest and SVM also work well with large numbers of features, so I think I would need to try them all.\n",
    "\n",
    "PCA could also be used to reduce dimensionality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Identify the most important characteristic predicting likelihood of being jailed before age 20.\n",
    "\n",
    "I've read in the Thinkful curriculum that Random Forest can do this on its own, but also using Recursive Feature Elimination (RFE) will rank the features by importance.\n",
    "\n",
    "# Feature Extraction with RFE\n",
    "from pandas import read_csv\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# load data\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "# feature extraction\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model, 3)\n",
    "fit = rfe.fit(X, Y)\n",
    "print(\"Num Features: %d\") % fit.n_features_\n",
    "print(\"Selected Features: %s\") % fit.support_\n",
    "print(\"Feature Ranking: %s\") % fit.ranking_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Implement a filter to “highlight” emails that might be important to the recipient\n",
    "\n",
    "Naive Bayes works on keyword searches, is great for large or small datasets and is very often used for filtering out spam emails and locating positive or negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.  You have 1000+ features\n",
    "\n",
    "PCA is the first model I would try especiallly if there is a high degree of multicollinearity. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.  Predict whether someone who adds items to their cart on a website will purchase the items.\n",
    "\n",
    "Logistic Regression and decision trees are both used for customer churn problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Your dataset dimensions are 982400 x 500\n",
    "\n",
    "Some of those features have got to go, I could use PCA, or I could use random forest and choose the most important features.  And logistic regression does better with huge samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Identify faces in an image.\n",
    "\n",
    "Facial recognition seems like it would be a regression problem, I've found a couple of instances online where linear SVM was used. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.  Predict which of three flavors of ice cream will be most popular with boys vs girls.\n",
    "\n",
    "This is a classification problem, logistic regression or SVM could be used, but random forest and SVM could also be used.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
